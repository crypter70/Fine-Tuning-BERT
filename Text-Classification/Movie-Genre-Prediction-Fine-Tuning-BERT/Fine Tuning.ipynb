{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf579e7c-2935-4c30-95c3-151c67bd9ebe",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fe5ec-a88a-459a-810f-3f815c508071",
   "metadata": {},
   "source": [
    "Project ini merupakan implementasi Transfer Learning dengan kasus prediksi genre film berdasarkan judul dan sinopsis. Project ini menggunakan pretrained model dari BERT yaitu bert-base-uncased (model yang teksnya sudah dalam bentuk lower case) beserta tokenizernya yang kemudian model akan dilatih ulang menggunakan dataset baru, istilah ini disebut fine tuning. Pretrained model dan tokenizer yang digunakan dalam project ini disimpan dalam direktori lokal, sehingga untuk menjalankan script ini diperlukan direktori yang menyimpan model, tokenizer, dan konfigurasinya yang dapat didownload pada URL berikut: https://huggingface.co/bert-base-uncased. Fine tuning menggunakan framework deep learning yaitu PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655cd04-09f2-4713-91ef-5b64e6f44374",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaad9ca8-063f-4418-8027-4e67719307cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# pip3 install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d208512-9d99-4cec-903f-7768fa8111e3",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55138629-8f42-4a34-8ecf-6cced3ecb1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 44978,\n",
       " 'movie_name': 'Super Me',\n",
       " 'synopsis': 'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.',\n",
       " 'genre': 4,\n",
       " 'final_text': 'movie name - super me, synopsis - a young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. selling them makes him rich.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_disk(\"cleaned_data/\")\n",
    "data = data.class_encode_column(\"genre\")\n",
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a85d21-5c89-4584-b1c2-562dd2013600",
   "metadata": {},
   "source": [
    "# Load Pretrained BERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e8a9a3-ac73-49ad-9d8b-6b2a7e93cf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased/\", use_fast=True, do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased/\",\n",
    "    num_labels=len(data['train'].features[\"genre\"]._int2str),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb43b6-c58c-4706-8ab2-c2f33aa72fb7",
   "metadata": {},
   "source": [
    "# Count Max Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbeadf3-5844-4af5-aa98-b43f3d25c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence len - 99\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for example in data['train']:\n",
    "    input_ids = tokenizer.encode(example['final_text'], add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print(f'Max sentence len - {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cecdde-f89d-42b3-b420-76f9f9300958",
   "metadata": {},
   "source": [
    "# Define Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c353ac2-eb77-4702-b895-397ac3fc65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.data[item][\"final_text\"])\n",
    "        target = int(self.data[item][\"genre\"])\n",
    "        inputs = self.tokenizer(text, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long).to(device),\n",
    "            \"attention_mask\": torch.tensor(mask, dtype=torch.long).to(device),\n",
    "            \"labels\": torch.tensor(target, dtype=torch.long).to(device),\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def train(ds):\n",
    "    ds_train = ds[\"train\"]\n",
    "    ds_test = ds[\"test\"]\n",
    "\n",
    "    temp_ds = ds_train.train_test_split(test_size=0.1, stratify_by_column=\"genre\")\n",
    "    ds_train = temp_ds[\"train\"]\n",
    "    ds_val = temp_ds[\"test\"]\n",
    "\n",
    "    train_dataset = ClassificationDataset(ds_train, tokenizer)\n",
    "    valid_dataset = ClassificationDataset(ds_val, tokenizer)\n",
    "    test_dataset = ClassificationDataset(ds_test, tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"model\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1, \n",
    "        optim=\"adamw_torch\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    preds = trainer.predict(test_dataset).predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    # generate result file\n",
    "    result = pd.DataFrame({\"id\": ds_test[\"id\"], \"genre\": preds})\n",
    "    result.loc[:, \"genre\"] = result.genre.apply(lambda x: ds_train.features[\"genre\"].int2str(x))\n",
    "    result.to_csv(f\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65599ea7-937c-4185-a211-37a90d4fe43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6075' max='6075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6075/6075 46:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.612900</td>\n",
       "      <td>1.601356</td>\n",
       "      <td>0.417963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
